---
layout: page
title: About
permalink: /about/
---

Hi, I'm Alex. I'm currently a Junior Data Scientist at a small tech company, and I'm beginning to apply to PhD programs in Computer Science. I'm interested in understanding how and why architectural decisions impact the performance of deep neural networks. In particular, I'm interested in understanding how model architecture influences the training dynamics of models, and the internal structures that form during training.

Previously, I worked on Vector Symbolic Architectures (VSAs) with Dr. Tim Oates and Dr. Edward Raff at the Univesity of Maryland, Baltimore County. VSAs represent symbols as high-dimensional vectors and define operators to manipulate them such that multiple symbols can be combined into a composite representation (known as a hypervector). The hypervector can then be "queried" using the operators in order to recover the original components [[1]](#1). Prior work at UMBC showed how VSAs can be integrated into deep learning systems, such as compressing the output layer when performing Extreme Multilabel Classification [[2]](#2).


### References

<a id="1">[1]</a> 
Schlegel, K., Neubert, P. & Protzel, P. A comparison of vector symbolic architectures. *Artif Intell Rev 55*, 4523â€“4555 (2022). [https://doi.org/10.1007/s10462-021-10110-3](https://doi.org/10.1007/s10462-021-10110-3)

<a id="2">[2]</a> 
Ashwinkumar Ganesan, Hang Gao, Sunil Gandhi, Edward Raff, Tim Oates, James Holt and Mark McLean. Learning with Holographic Reduced Representations. *Advances in Neural Information Processing Systems 34*, 25606-25620 (2021). [https://proceedings.neurips.cc/paper_files/paper/2021/file/d71dd235287466052f1630f31bde7932-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2021/file/d71dd235287466052f1630f31bde7932-Paper.pdf)